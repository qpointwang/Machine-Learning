{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet.gluon import data as gdata\n",
    "import sys\n",
    "import time\n",
    "import mxnet as mx\n",
    "mnist_train = gdata.vision.FashionMNIST(train=True)\n",
    "mnist_test = gdata.vision.FashionMNIST(train=False)\n",
    "len(mnist_train), len(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fashion_mnist(images, labels):\n",
    "    d2l.use_svg_display()\n",
    "    # 这⾥的_表⽰我们忽略（不使⽤）的变量\n",
    "    _, figs = d2l.plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.reshape((28, 28)).asnumpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist_train[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 9, 6, 0, 3, 4, 4, 5, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pullover',\n",
       " 'ankle boot',\n",
       " 'shirt',\n",
       " 't-shirt',\n",
       " 'dress',\n",
       " 'coat',\n",
       " 'coat',\n",
       " 'sandal',\n",
       " 'coat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fashion_mnist_labels(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "# 我们通过ToTensor实例将图像数据从uint8格式变换成32位浮点数格式，并除以255使得所有像素的数值均在0到1之间。\n",
    "transformer = gdata.vision.transforms.ToTensor()\n",
    "if sys.platform.startswith('win'):\n",
    "    num_workers = 0 # 0表⽰不⽤额外的进程来加速读取数据\n",
    "else:\n",
    "    num_workers = 4\n",
    "train_iter = gdata.DataLoader(mnist_train.transform_first(transformer), batch_size, \n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "test_iter = gdata.DataLoader(mnist_test.transform_first(transformer), batch_size, \n",
    "                             shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'win32'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import d2lzh as d2l\n",
    "from mxnet import autograd, nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "W = nd.random.normal(scale=0.01, shape=(num_inputs, num_outputs), ctx=mx.gpu())\n",
    "b = nd.zeros(num_outputs, ctx=mx.gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.attach_grad()\n",
    "b.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[5. 7. 9.]]\n",
       " <NDArray 1x3 @cpu(0)>, \n",
       " [[ 6.]\n",
       "  [15.]]\n",
       " <NDArray 2x1 @cpu(0)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.array([[1, 2, 3], [4, 5, 6]])\n",
    "X.sum(axis=0, keepdims=True), X.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(axis=1, keepdims=True)\n",
    "    return X_exp / partition # 这⾥应⽤了⼴播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[0.41841468 0.21212995 0.17643595 0.15238647 0.04063296]\n",
       "  [0.2188216  0.02668872 0.0043691  0.07917226 0.6709483 ]]\n",
       " <NDArray 2x5 @cpu(0)>, \n",
       " [1. 1.]\n",
       " <NDArray 2 @cpu(0)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = nd.random.normal(shape=(2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(nd.dot(X.reshape((-1, num_inputs)), W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -nd.pick(y_hat, y).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(axis=1) == y.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        y = y.astype('float32')\n",
    "        acc_sum += (net(X.copyto(mx.gpu())).argmax(axis=1) == y.copyto(mx.gpu())).sum().asscalar()\n",
    "        n += y.size\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_iter, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y_cpu in train_iter:\n",
    "            y = y_cpu.copyto(mx.gpu())\n",
    "            with autograd.record():\n",
    "                y_hat = net(X.copyto(mx.gpu()))\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            if trainer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                trainer.step(batch_size) # “softmax回归的简洁实现”⼀节将⽤到\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7883, train acc 0.747, test acc 0.804\n",
      "epoch 2, loss 0.5727, train acc 0.811, test acc 0.827\n",
      "epoch 3, loss 0.5293, train acc 0.823, test acc 0.827\n",
      "epoch 4, loss 0.5047, train acc 0.830, test acc 0.833\n",
      "epoch 5, loss 0.4899, train acc 0.835, test acc 0.834\n"
     ]
    }
   ],
   "source": [
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax回归的简洁实现\n",
    "\n",
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpu(0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx = d2l.try_gpu()\n",
    "ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.7870, train acc 0.750, test acc 0.805\n",
      "epoch 2, loss 0.5729, train acc 0.811, test acc 0.824\n",
      "epoch 3, loss 0.5287, train acc 0.823, test acc 0.829\n",
      "epoch 4, loss 0.5055, train acc 0.829, test acc 0.832\n",
      "epoch 5, loss 0.4887, train acc 0.834, test acc 0.841\n",
      "epoch 6, loss 0.4781, train acc 0.839, test acc 0.844\n",
      "epoch 7, loss 0.4702, train acc 0.839, test acc 0.845\n",
      "epoch 8, loss 0.4620, train acc 0.842, test acc 0.846\n",
      "epoch 9, loss 0.4570, train acc 0.844, test acc 0.847\n",
      "epoch 10, loss 0.4516, train acc 0.845, test acc 0.849\n",
      "epoch 11, loss 0.4475, train acc 0.846, test acc 0.849\n",
      "epoch 12, loss 0.4436, train acc 0.848, test acc 0.850\n",
      "epoch 13, loss 0.4396, train acc 0.849, test acc 0.849\n",
      "epoch 14, loss 0.4370, train acc 0.850, test acc 0.850\n",
      "epoch 15, loss 0.4339, train acc 0.851, test acc 0.852\n",
      "epoch 16, loss 0.4319, train acc 0.852, test acc 0.852\n",
      "epoch 17, loss 0.4301, train acc 0.853, test acc 0.854\n",
      "epoch 18, loss 0.4283, train acc 0.852, test acc 0.850\n",
      "epoch 19, loss 0.4257, train acc 0.854, test acc 0.852\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.1})\n",
    "\n",
    "num_epochs = 20\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
